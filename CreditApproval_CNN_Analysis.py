# -*- coding: utf-8 -*-
"""CreditApproval_CNN_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rm99ctuURr0qcEQXaNuwsVRFP5M37hUD

# ** Credit Approval CNN Analysis: Illustrate the Step-by-Step Process of Applying Convolutional Operations**

# **Created by: Preksha Shah**

# **Domain: Banking**

---

## **Problem Statement:**
The task is to illustrate the step-by-step process of applying convolutional operations on a dataset to predict which people are successful in applying for a credit card.

---

## **1. Loading and Preparing the Dataset**

### **1.1 Import Necessary Libraries**
"""

# Import the necessary libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### **1.2 Load the Dataset**"""

# Load the dataset
dataframe = pd.read_csv('/content/clean_dataset.csv')

# Read the dataset into a pandas DataFrame
df = pd.read_csv('clean_dataset.csv')

# Display the first 5 rows of the dataset
print(df.head())

"""## **2. Basic Data Exploration**

### **2.1 Basic Data Exploration**

"""

# Display basic information about the dataset
print("\nDataset Info:")
print(df.info())

"""#### **Inference:**

1. **Gender**:
   - Dataset contains gender information encoded as 0 (Female) and 1 (Male).

2. **Age**:
   - Age distribution is broad, indicating varied age groups among applicants.

3. **Debt**:
   - Debt values are scaled, providing a normalized range.

4. **Married**:
   - Marital status encoded as 0 (Single/Divorced/etc) and 1 (Married).

5. **Bank Customer**:
   - Indicates whether the applicant has a bank account, encoded as 0 or 1.

6. **Industry**:
   - Contains information on the job sector of the current or most recent job.

7. **Ethnicity**:
   - Ethnicity of the applicants.

8. **Years Employed**:
   - Years employed feature indicating the duration of employment.

9. **Prior Default**:
   - Indicates prior default status, encoded as 0 or 1.

10. **Employed**:
    - Employment status encoded as 0 (Not Employed) and 1 (Employed).

## **4. Data Preprocessing**
"""

# Display summary statistics of the dataset
print("\nSummary Statistics:")
print(df.describe())

"""####**Inferences:**

1. **Gender**:
   - **Mean**: 0.696, indicating that approximately 70% of the dataset consists of one gender (presumably male if coded as 1).
   - **Range**: The values are binary (0 or 1), so the data is equally distributed between genders.

2. **Age**:
   - **Mean**: 31.514 years, suggesting the average age of individuals in the dataset.
   - **Range**: The age ranges from 13.75 to 80.25 years, with a standard deviation of 11.860, indicating a diverse age distribution.
   - **Quartiles**: The middle 50% of individuals are between 22.67 and 37.71 years old.

3. **Debt**:
   - **Mean**: 4.759, which represents the average debt value, likely in thousands or another unit of measurement.
   - **Range**: The debt ranges from 0 to 28, with a significant spread as indicated by the standard deviation (4.978).
   - **Quartiles**: 25% of individuals have a debt of 1 or less, and 75% have a debt of 7.21 or less.

4. **Married**:
   - **Mean**: 0.761, suggesting that around 76% of the dataset consists of married individuals.
   - **Range**: The values are binary (0 or 1), indicating a majority of married individuals.

5. **BankCustomer**:
   - **Mean**: 0.764, meaning about 76% of the dataset are bank customers.
   - **Range**: The values are binary (0 or 1), showing a majority of bank customers.

6. **YearsEmployed**:
   - **Mean**: 2.223 years, indicating the average duration of employment.
   - **Range**: Employment ranges from 0 to 28.5 years, showing variability in work experience.
   - **Quartiles**: The middle 50% of individuals have between 0.17 and 2.63 years of employment.

7. **PriorDefault**:
   - **Mean**: 0.523, suggesting that around 52% of individuals have a history of prior default.
   - **Range**: The values are binary (0 or 1), indicating a moderate incidence of prior defaults.

8. **Employed**:
   - **Mean**: 0.428, meaning about 43% of individuals are currently employed.
   - **Range**: The values are binary (0 or 1), showing less than half of the individuals are employed.

9. **CreditScore**:
   - **Mean**: 2.4, indicating the average credit score, with values likely ranging from 0 to 67.
   - **Range**: The credit score ranges from 0 to 67, with a large variability in the data.
   - **Quartiles**: 25% of individuals have a credit score of 0, while 75% have a score of 3 or less.

10. **DriversLicense**:
    - **Mean**: 0.458, suggesting about 46% of individuals have a driver's license.
    - **Range**: The values are binary (0 or 1), indicating a significant proportion of individuals without a license.

11. **ZipCode**:
    - **Mean**: 180.548, representing the average zip code value.
    - **Range**: The zip code ranges from 0 to 2000, showing a broad geographical distribution.
    - **Quartiles**: The middle 50% of zip codes fall between 60 and 272.

12. **Income**:
    - **Mean**: 1017.386, indicating the average income, with significant variation.
    - **Range**: Income ranges from 0 to 100,000, suggesting a wide range of income levels.
    - **Quartiles**: 25% of individuals have an income of 0, while 75% earn 395.5 or less.

13. **Approved**:
    - **Mean**: 0.445, meaning approximately 45% of individuals were approved for the credit or loan.
    - **Range**: The values are binary (0 or 1), indicating that less than half of the individuals were approved.

####**Summary**
- The dataset contains a diverse range of ages, debts, and income levels.
- There is a higher proportion of married and bank customers in the dataset.
- A significant portion of the dataset has prior defaults and varying levels of employment.
- Credit scores and income levels show substantial variability.
- The approval rate is around 45%, suggesting a moderate rate of credit approval.

"""

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Convert categorical columns to numerical format using one-hot encoding
df_encoded = pd.get_dummies(df, columns=['Gender', 'Married', 'BankCustomer', 'Industry', 'Ethnicity', 'PriorDefault', 'Employed', 'Citizen'])

"""### **4.1 Split the Dataset**"""

# Define features (X) and labels (y)
X = df_encoded.drop(columns=['Approved'])
y = df_encoded['Approved']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### **4.2 Standardize the Data**"""

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""### **4.3 Reshape the Data**"""

# Reshape data to fit the input requirements of a CNN
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

"""## **5. Build the Convolutional Neural Network (CNN) Model**

### **5.1 Initialize the Model and add Convolutional Layers**

"""

# Build the Convolutional Neural Network (CNN) Model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.3))

model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.3))

model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.3))

model.add(Flatten())
model.add(Dense(units=128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))
model.add(Dropout(0.5))
model.add(Dense(units=1, activation='sigmoid'))

"""## **6. Compile the Model**"""

# Compile the Model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])

# Set up Early Stopping, Model Checkpoint, and ReduceLROnPlateau
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)

"""## **7. Train the Model**"""

# Train the Model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, model_checkpoint, reduce_lr])

"""####**Inference of the Model Training Progress:**

- **Initial Epochs (1-10):**
  - Accuracy and validation accuracy show improvement, but validation accuracy is lower than training accuracy, indicating potential overfitting.

- **Mid Training (11-20):**
  - Training accuracy improves significantly, reaching around 84.81% by epoch 20. Validation accuracy also increases, reaching about 78.38%. There is still a noticeable gap between training and validation accuracy.

- **Later Epochs (21-30):**
  - Both training and validation accuracies improve further. By epoch 30, training accuracy reaches around 89.34%, and validation accuracy is stable around 86.49%. The model shows a decreasing trend in validation loss, suggesting improved generalization.

- **Recent Epochs (31-38):**
  - Accuracy levels stabilize, with the model achieving around 88.29% accuracy consistently. Validation accuracy also hovers around 87.39%, showing strong performance and good generalization.


## **8. Evaluate the Model**
"""

# Evaluate the Model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy:.4f}')

"""####**Inferences:**
The model shows steady improvement in both training and validation accuracies, with recent epochs achieving high accuracy and low loss values. The validation accuracy has stabilized around 87-88%, indicating that the model has likely reached a good balance between fit and generalization.

## **9. Visualize the Training Process**
"""

# Visualize the Training Process
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy over Epochs')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.tight_layout()
plt.show()

"""### **Inferences from Training Process Visualization**

**Accuracy:**
- **Training Accuracy:** Generally increases over epochs and stabilizes, showing the model's improvement on training data.
- **Validation Accuracy:** Should ideally follow a similar trend. If it lags significantly behind training accuracy or starts to decrease, it may indicate overfitting.

**Loss:**
- **Training Loss:** Typically decreases over time, reflecting the model's learning progress.
- **Validation Loss:** Should also decrease. If it diverges from training loss, it could signal overfitting.

**Overall:**
- **Good Performance:** Both accuracy metrics increase and losses decrease, indicating effective learning.
- **Overfitting:** High training accuracy with decreasing training loss but increasing validation loss suggests overfitting.
- **Underfitting:** Low accuracy and high loss across both datasets suggest underfitting.


---

### **10. Key Achievements**

1. **Successful Convolutional Model Implementation:** Demonstrated the step-by-step process of building and applying a Convolutional Neural Network (CNN) to predict credit card application success.

2. **Data Preparation and Preprocessing:** Effectively handled and preprocessed a diverse dataset, including encoding categorical variables and standardizing numerical features for CNN input.

3. **Model Training and Evaluation:** Achieved a high validation accuracy of approximately 87-88% with a well-balanced model, indicating strong generalization and effective learning.

4. **Training Process Visualization:** Provided clear visual insights into the model's training progress, showing improvements in accuracy and reductions in loss over epochs.

### **11. Conclusion**

The project successfully illustrates the application of convolutional operations for predicting credit card application success. The implemented CNN model showed promising performance, with consistent improvements in both training and validation metrics. The visualizations of accuracy and loss trends validate the model's effectiveness and demonstrate its ability to generalize well to new data. This approach provides a robust foundation for predictive modeling in the banking domain using convolutional neural networks.

---
---
"""